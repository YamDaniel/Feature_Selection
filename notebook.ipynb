{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = '/Users/yamdaniel/Feature_Selection/datasets/breast_cancer/SCANB.csv'\n",
    "file_path2 = '/Users/yamdaniel/Feature_Selection/datasets/breast_cancer/sampleinfo_SCANB_t.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename1, filename2):\n",
    "    x = pd.read_csv(filename1)\n",
    "    x = x.set_index(x.columns[0])\n",
    "    y = pd.read_csv(filename2)['PAM50'] \n",
    "    return x.T, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_data(file_path1, file_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, L1=0.01, L2=0.01):\n",
    "        super(SoftmaxClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(num_features, num_classes, bias=True)  # Y = W*X + b + L1*|W| + L2*W^2 \n",
    "        self.L1 = L1 # shape: (num_classes, num_features)\n",
    "        self.L2 = L2 # shape: (num_classes, num_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.fc(x), dim=1) \n",
    "\n",
    "    def group_loss(self):\n",
    "\n",
    "        W = self.fc.weight  # Shape: (num_classes, num_features)\n",
    "\n",
    "        L1 = torch.max(torch.abs(W), dim=1)[0]  \n",
    "        L1 = self.L1 * torch.mean(L1)\n",
    "\n",
    "        L2 = torch.sum(W * W, dim=1)\n",
    "        L2 = self.L2 * torch.mean(L2)\n",
    "\n",
    "        return L1 + L2\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        \"\"\"\n",
    "        Computes the loss for a given batch.\n",
    "        \"\"\"\n",
    "        inputs, labels = batch  # Unpack batch (features, labels)\n",
    "        outputs = self.forward(inputs)  # Forward pass\n",
    "        criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss += self.group_loss()  # Add regularization loss\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Configures the optimizer for training.\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "# Cross-validation function\n",
    "def cross_validate(model, X, y, k=5, batch_size=32, epochs=10):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)  # K-fold cross-validation\n",
    "    fold_accuracies = []  # To store accuracy for each fold\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        print(f\"Training fold {fold + 1}/{k}\")\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "        # Initialize model and optimizer\n",
    "        model_instance = model(X_train.shape[1], len(np.unique(y)))  # num_features, num_classes\n",
    "        optimizer = model_instance.configure_optimizers()\n",
    "\n",
    "        # Train the model on this fold\n",
    "        for epoch in range(epochs):\n",
    "            model_instance.train()\n",
    "            permutation = torch.randperm(X_train_tensor.size(0))\n",
    "\n",
    "            for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "                batch_indices = permutation[i:i+batch_size]\n",
    "                batch_X, batch_y = X_train_tensor[batch_indices], y_train_tensor[batch_indices]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = model_instance.training_step((batch_X, batch_y))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        model_instance.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model_instance(X_val_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            accuracy = accuracy_score(y_val, predicted.numpy())  # Compute accuracy\n",
    "            fold_accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"Accuracy for fold {fold + 1}: {accuracy:.4f}\")\n",
    "\n",
    "    # Compute the average accuracy across all folds\n",
    "    avg_accuracy = np.mean(fold_accuracies)\n",
    "    print(f\"\\nAverage accuracy across {k} folds: {avg_accuracy:.4f}\")\n",
    "    return avg_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
